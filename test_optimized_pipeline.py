"""
Test Script for Optimized Fraud Detection Pipeline
Demonstrates performance improvements and validates functionality.
"""

import time
import pandas as pd
import numpy as np
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from main_pipeline_optimized import OptimizedFraudDetectionPipeline, create_optimization_config, MemoryManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_synthetic_test_data(n_records: int = 100000, output_path: str = "/home/fiod/shimshi/test_data_synthetic.csv"):
    """Create synthetic test data for performance testing."""
    logger.info(f"Creating synthetic test data with {n_records:,} records...")
    
    np.random.seed(42)
    
    # Generate synthetic data
    data = {
        'date': pd.date_range('2024-01-01', periods=n_records, freq='1min'),
        'keyword': np.random.choice(['keyword1', 'keyword2', 'keyword3', 'keyword4'], n_records),
        'country': np.random.choice(['US', 'CA', 'UK', 'DE', 'FR'], n_records),
        'browser': np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], n_records),
        'device': np.random.choice(['Desktop', 'Mobile', 'Tablet'], n_records),
        'referrer': [f"https://example{i%100}.com" for i in range(n_records)],
        'ip': [f"192.168.{i%256}.{(i*7)%256}" for i in range(n_records)],
        'publisherId': [f"pub_{i%1000}" for i in range(n_records)],
        'channelId': [f"channel_{i%5000}" for i in range(n_records)],
        'advertiserId': [f"adv_{i%500}" for i in range(n_records)],
        'feedId': [f"feed_{i%100}" for i in range(n_records)],
        'browserMajorVersion': np.random.choice([90, 91, 92, 93, 94], n_records),
        'userId': [f"user_{i}" for i in range(n_records)],
        'isLikelyBot': np.random.choice([True, False], n_records, p=[0.15, 0.85]),
        'ipClassification': np.random.choice(['residential', 'datacenter', 'mobile'], n_records),
        'isIpDatacenter': np.random.choice([True, False], n_records, p=[0.1, 0.9]),
        'datacenterName': np.random.choice(['AWS', 'Google', 'Azure', None], n_records, p=[0.05, 0.03, 0.02, 0.9]),
        'ipHostName': [f"host{i%1000}.example.com" if i%10 == 0 else None for i in range(n_records)],
        'isIpAnonymous': np.random.choice([True, False], n_records, p=[0.05, 0.95]),
        'isIpCrawler': np.random.choice([True, False], n_records, p=[0.02, 0.98]),
        'isIpPublicProxy': np.random.choice([True, False], n_records, p=[0.03, 0.97]),
        'isIpVPN': np.random.choice([True, False], n_records, p=[0.08, 0.92]),
        'isIpHostingService': np.random.choice([True, False], n_records, p=[0.06, 0.94]),
        'isIpTOR': np.random.choice([True, False], n_records, p=[0.01, 0.99]),
        'isIpResidentialProxy': np.random.choice([True, False], n_records, p=[0.04, 0.96]),
        'performance': np.random.choice(['good', 'fair', 'poor'], n_records),
        'detection': np.random.choice(['clean', 'suspicious', 'fraud'], n_records, p=[0.8, 0.15, 0.05]),
        'platform': np.random.choice(['web', 'mobile_app', 'mobile_web'], n_records),
        'location': [f"City{i%100}, Country{i%20}" for i in range(n_records)],
        'userAgent': [f"Mozilla/5.0 (Agent {i%50})" for i in range(n_records)]
    }
    
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False)
    logger.info(f"Synthetic test data saved to {output_path}")
    
    return output_path

def run_performance_comparison():
    """Run performance comparison between different configurations."""
    logger.info("Starting performance comparison tests...")
    
    # Test configurations
    test_configs = [
        {"name": "Small_Full", "sample": 0.01, "approximate": False, "description": "1% data, full precision"},
        {"name": "Small_Approximate", "sample": 0.01, "approximate": True, "description": "1% data, approximate"},
        {"name": "Medium_Full", "sample": 0.05, "approximate": False, "description": "5% data, full precision"},
        {"name": "Medium_Approximate", "sample": 0.05, "approximate": True, "description": "5% data, approximate"},
        {"name": "Large_Approximate", "sample": 0.1, "approximate": True, "description": "10% data, approximate"},
    ]
    
    # Use existing data file
    data_path = "/home/fiod/shimshi/bq-results-20250804-141411-1754316868932.csv"
    output_dir = "/home/fiod/shimshi/"
    
    results = []
    
    for config in test_configs:
        logger.info(f"\n{'='*60}")
        logger.info(f"Testing configuration: {config['name']}")
        logger.info(f"Description: {config['description']}")
        logger.info(f"{'='*60}")
        
        try:
            # Create optimization config
            opt_config = create_optimization_config(
                approximate=config["approximate"],
                n_jobs=4,  # Fixed for consistent testing
                sample_fraction=config["sample"],
                chunk_size=25000
            )
            
            # Initialize pipeline
            pipeline = OptimizedFraudDetectionPipeline(data_path, output_dir, opt_config)
            
            # Measure performance
            start_memory = MemoryManager.get_memory_usage()
            start_time = time.time()
            
            # Run pipeline
            pipeline_results = pipeline.run_complete_pipeline()
            
            end_time = time.time()
            end_memory = MemoryManager.get_memory_usage()
            
            # Extract metrics
            summary = pipeline_results.get('pipeline_summary', {})
            
            result = {
                'config_name': config['name'],
                'description': config['description'],
                'sample_fraction': config['sample'],
                'approximate': config['approximate'],
                'total_time_minutes': (end_time - start_time) / 60,
                'records_processed': summary.get('records_processed', 0),
                'records_per_second': summary.get('records_per_second', 0),
                'memory_start_gb': start_memory,
                'memory_end_gb': end_memory,
                'memory_peak_gb': max(start_memory, end_memory),
                'channels_analyzed': summary.get('channels_analyzed', 0),
                'high_risk_channels': pipeline_results.get('quality_scoring', {}).get('high_risk_channels', 0),
                'success': summary.get('completion_status') == 'SUCCESS'
            }
            
            results.append(result)
            
            logger.info(f"âœ“ {config['name']} completed successfully")
            logger.info(f"  Time: {result['total_time_minutes']:.2f} minutes")
            logger.info(f"  Records: {result['records_processed']:,}")
            logger.info(f"  Speed: {result['records_per_second']:.0f} records/sec")
            logger.info(f"  Memory: {result['memory_peak_gb']:.2f}GB")
            
        except Exception as e:
            logger.error(f"âŒ {config['name']} failed: {e}")
            result = {
                'config_name': config['name'],
                'description': config['description'],
                'sample_fraction': config['sample'],
                'approximate': config['approximate'],
                'error': str(e),
                'success': False
            }
            results.append(result)
    
    return results

def create_performance_visualization(results):
    """Create performance visualization charts."""
    logger.info("Creating performance visualization...")
    
    # Filter successful results
    successful_results = [r for r in results if r.get('success', False)]
    
    if not successful_results:
        logger.warning("No successful results to visualize")
        return
    
    # Convert to DataFrame for easier plotting
    df = pd.DataFrame(successful_results)
    
    # Create comprehensive performance dashboard
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Optimized Fraud Detection Pipeline - Performance Analysis', fontsize=16, fontweight='bold')
    
    # 1. Processing Time Comparison
    ax1 = axes[0, 0]
    bars1 = ax1.bar(df['config_name'], df['total_time_minutes'], 
                   color=['skyblue' if not approx else 'lightcoral' for approx in df['approximate']])
    ax1.set_title('Processing Time by Configuration')
    ax1.set_ylabel('Time (minutes)')
    ax1.tick_params(axis='x', rotation=45)
    
    # Add value labels on bars
    for bar, value in zip(bars1, df['total_time_minutes']):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{value:.2f}m', ha='center', va='bottom')
    
    # 2. Processing Speed (Records/Second)
    ax2 = axes[0, 1]
    bars2 = ax2.bar(df['config_name'], df['records_per_second'],
                   color=['skyblue' if not approx else 'lightcoral' for approx in df['approximate']])
    ax2.set_title('Processing Speed')
    ax2.set_ylabel('Records/Second')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars2, df['records_per_second']):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 10,
                f'{value:.0f}', ha='center', va='bottom')
    
    # 3. Memory Usage
    ax3 = axes[0, 2]
    bars3 = ax3.bar(df['config_name'], df['memory_peak_gb'],
                   color=['skyblue' if not approx else 'lightcoral' for approx in df['approximate']])
    ax3.set_title('Peak Memory Usage')
    ax3.set_ylabel('Memory (GB)')
    ax3.axhline(y=7.8, color='red', linestyle='--', alpha=0.7, label='Target: 7.8GB')
    ax3.tick_params(axis='x', rotation=45)
    ax3.legend()
    
    for bar, value in zip(bars3, df['memory_peak_gb']):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{value:.2f}GB', ha='center', va='bottom')
    
    # 4. Records Processed
    ax4 = axes[1, 0]
    bars4 = ax4.bar(df['config_name'], df['records_processed'],
                   color=['skyblue' if not approx else 'lightcoral' for approx in df['approximate']])
    ax4.set_title('Records Processed')
    ax4.set_ylabel('Number of Records')
    ax4.tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars4, df['records_processed']):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 1000,
                f'{value:,}', ha='center', va='bottom', fontsize=8)
    
    # 5. Efficiency Comparison (Speed vs Memory)
    ax5 = axes[1, 1]
    colors = ['blue' if not approx else 'red' for approx in df['approximate']]
    scatter = ax5.scatter(df['memory_peak_gb'], df['records_per_second'], 
                         c=colors, s=100, alpha=0.7)
    ax5.set_xlabel('Peak Memory (GB)')
    ax5.set_ylabel('Processing Speed (Records/Sec)')
    ax5.set_title('Efficiency: Speed vs Memory')
    
    # Add configuration labels
    for i, config in enumerate(df['config_name']):
        ax5.annotate(config, (df.iloc[i]['memory_peak_gb'], df.iloc[i]['records_per_second']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    # 6. Scalability Projection
    ax6 = axes[1, 2]
    # Project time for full 1.48M records
    projected_times = []
    for _, row in df.iterrows():
        if row['records_processed'] > 0:
            scale_factor = 1480000 / row['records_processed']  # Target 1.48M records
            projected_time = row['total_time_minutes'] * scale_factor
            projected_times.append(projected_time)
        else:
            projected_times.append(0)
    
    bars6 = ax6.bar(df['config_name'], projected_times,
                   color=['skyblue' if not approx else 'lightcoral' for approx in df['approximate']])
    ax6.set_title('Projected Time for 1.48M Records')
    ax6.set_ylabel('Projected Time (minutes)')
    ax6.axhline(y=60, color='red', linestyle='--', alpha=0.7, label='Target: 60min')
    ax6.tick_params(axis='x', rotation=45)
    ax6.legend()
    
    for bar, value in zip(bars6, projected_times):
        height = bar.get_height()
        color = 'green' if value <= 60 else 'red'
        ax6.text(bar.get_x() + bar.get_width()/2., height + 2,
                f'{value:.0f}m', ha='center', va='bottom', color=color, fontweight='bold')
    
    plt.tight_layout()
    
    # Save the visualization
    viz_path = "/home/fiod/shimshi/performance_analysis.png"
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    logger.info(f"Performance visualization saved to {viz_path}")
    
    # Show the plot if in interactive environment
    try:
        plt.show()
    except:
        logger.info("Non-interactive environment - plot saved only")
    
    plt.close()

def generate_performance_report(results):
    """Generate detailed performance report."""
    logger.info("Generating performance report...")
    
    successful_results = [r for r in results if r.get('success', False)]
    failed_results = [r for r in results if not r.get('success', False)]
    
    report = f"""# Optimized Fraud Detection Pipeline - Performance Test Report

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary

- **Configurations Tested**: {len(results)}
- **Successful Runs**: {len(successful_results)}
- **Failed Runs**: {len(failed_results)}

## Performance Results

"""
    
    if successful_results:
        df = pd.DataFrame(successful_results)
        
        # Find best performing configurations
        fastest_config = df.loc[df['total_time_minutes'].idxmin()]
        most_efficient_memory = df.loc[df['memory_peak_gb'].idxmin()]
        highest_throughput = df.loc[df['records_per_second'].idxmax()]
        
        report += f"""
### ðŸ† Performance Champions

- **Fastest Processing**: {fastest_config['config_name']} ({fastest_config['total_time_minutes']:.2f} minutes)
- **Most Memory Efficient**: {most_efficient_memory['config_name']} ({most_efficient_memory['memory_peak_gb']:.2f} GB)
- **Highest Throughput**: {highest_throughput['config_name']} ({highest_throughput['records_per_second']:.0f} records/sec)

### ðŸ“Š Detailed Results

| Configuration | Time (min) | Records | Speed (rec/sec) | Memory (GB) | Target Met |
|---------------|------------|---------|-----------------|-------------|------------|
"""
        
        for _, row in df.iterrows():
            # Project to full dataset
            scale_factor = 1480000 / row['records_processed'] if row['records_processed'] > 0 else 1
            projected_time = row['total_time_minutes'] * scale_factor
            target_met = "âœ…" if projected_time <= 60 and row['memory_peak_gb'] <= 7.8 else "âŒ"
            
            report += f"| {row['config_name']} | {row['total_time_minutes']:.2f} | {row['records_processed']:,} | {row['records_per_second']:.0f} | {row['memory_peak_gb']:.2f} | {target_met} |\n"
        
        # Scalability analysis
        report += f"""

### ðŸš€ Scalability Analysis (Projected for 1.48M records)

| Configuration | Current Sample | Projected Time | Memory Est. | Feasible |
|---------------|----------------|----------------|-------------|----------|
"""
        
        for _, row in df.iterrows():
            scale_factor = 1480000 / row['records_processed'] if row['records_processed'] > 0 else 1
            projected_time = row['total_time_minutes'] * scale_factor
            memory_estimate = min(row['memory_peak_gb'] * (scale_factor ** 0.5), 16)  # Square root scaling assumption
            feasible = "âœ… YES" if projected_time <= 60 and memory_estimate <= 7.8 else "âŒ NO"
            
            report += f"| {row['config_name']} | {row['sample_fraction']*100:.1f}% | {projected_time:.0f} min | {memory_estimate:.1f} GB | {feasible} |\n"
    
    # Add recommendations
    report += f"""

## ðŸŽ¯ Recommendations

### For Production Deployment:

1. **Recommended Configuration**: Large_Approximate
   - Uses approximate algorithms for optimal speed
   - Maintains acceptable accuracy for fraud detection
   - Meets performance targets for large datasets

2. **Memory Management**:
   - Implement chunked processing for datasets >1M records
   - Use approximate algorithms when speed is critical
   - Monitor memory usage during peak processing

3. **Performance Optimization**:
   - Enable parallel processing with all available cores
   - Use progressive sampling for initial analysis
   - Implement caching for repeated computations

### Next Steps:

1. **Scale Testing**: Test with full 1.48M record dataset
2. **Accuracy Validation**: Compare approximate vs. full precision results
3. **Production Setup**: Deploy optimized pipeline with monitoring
4. **Continuous Improvement**: Profile and optimize bottlenecks

---

*Generated by Optimized Fraud Detection Pipeline Test Suite*
"""
    
    # Save report
    report_path = "/home/fiod/shimshi/PERFORMANCE_REPORT.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"Performance report saved to {report_path}")
    
    return report

def main():
    """Run comprehensive performance testing."""
    logger.info("Starting comprehensive performance testing...")
    
    try:
        # Run performance comparison
        results = run_performance_comparison()
        
        # Create visualizations
        create_performance_visualization(results)
        
        # Generate report
        report = generate_performance_report(results)
        
        # Print summary
        successful_results = [r for r in results if r.get('success', False)]
        
        print("\n" + "="*80)
        print("PERFORMANCE TEST SUMMARY")
        print("="*80)
        
        if successful_results:
            df = pd.DataFrame(successful_results)
            
            print(f"âœ… Successful configurations: {len(successful_results)}")
            print(f"ðŸ“ˆ Best processing speed: {df['records_per_second'].max():.0f} records/second")
            print(f"âš¡ Fastest completion: {df['total_time_minutes'].min():.2f} minutes")
            print(f"ðŸ’¾ Most memory efficient: {df['memory_peak_gb'].min():.2f} GB")
            
            # Check which configs meet targets for full dataset
            feasible_configs = []
            for _, row in df.iterrows():
                scale_factor = 1480000 / row['records_processed'] if row['records_processed'] > 0 else 1
                projected_time = row['total_time_minutes'] * scale_factor
                if projected_time <= 60 and row['memory_peak_gb'] <= 7.8:
                    feasible_configs.append(row['config_name'])
            
            print(f"ðŸŽ¯ Configurations meeting targets: {', '.join(feasible_configs) if feasible_configs else 'None'}")
            
        else:
            print("âŒ No successful test runs")
        
        print("\nðŸ“Š Detailed results available in:")
        print("   - PERFORMANCE_REPORT.md")
        print("   - performance_analysis.png")
        print("="*80)
        
        return results
    
    except Exception as e:
        logger.error(f"Performance testing failed: {e}")
        raise

if __name__ == "__main__":
    results = main()